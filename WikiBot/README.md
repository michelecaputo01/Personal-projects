# Wikibot
In this folder, some of the most common Large Language Models are fine-tuned using the well-known SQuAD 2.0 dataset. After some exploratory analysis, three LLMs are selected for fine-tuning:
- BERT, developed by Google in 2018;
- RoBERTa, developed by Facebook, an evolution of BERT;
- DistilBERT, developed by Hugging Face, an optimized version of BERT (about 40% fewer parameters).
After fine-tuning, both the raw models and the trained models are analyzed, demonstrating the effectiveness of fine-tuning and the quality of the dataset. Finally, several Q&A examples are provided.
